{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f8f589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict, load_from_disk\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from pathlib import Path\n",
    "import os\n",
    "import dotenv\n",
    "import gc\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75cf77e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from sacremoses) (2025.11.3)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from sacremoses) (8.3.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from sacremoses) (1.5.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sacremoses) (4.67.1)\n",
      "Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sacremoses\n",
      "Successfully installed sacremoses-0.1.1\n"
     ]
    }
   ],
   "source": [
    "# only run on colab\n",
    "%pip install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8af070d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Ro-Md-En-MT-Analysis'...\n",
      "remote: Enumerating objects: 30044, done.\u001b[K\n",
      "remote: Counting objects: 100% (30044/30044), done.\u001b[K\n",
      "remote: Compressing objects: 100% (30017/30017), done.\u001b[K\n",
      "remote: Total 30044 (delta 19), reused 30037 (delta 14), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (30044/30044), 6.71 MiB | 11.18 MiB/s, done.\n",
      "Resolving deltas: 100% (19/19), done.\n",
      "Updating files: 100% (30012/30012), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/TanaseVictorFlavian/Ro-Md-En-MT-Analysis.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0fbcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Data\n",
    "parallel_corpus_dir = Path.cwd() / \"parallel_corpus\"\n",
    "\n",
    "if parallel_corpus_dir.exists() is False:\n",
    "    # fallback for using colab extension in vscode\n",
    "    parallel_corpus_dir = Path(\"/content/Ro-Md-En-MT-Analysis/parallel_corpus\")\n",
    "\n",
    "json_files = list(parallel_corpus_dir.glob(\"*.json\"))\n",
    "data_list = []\n",
    "\n",
    "for file_path in json_files:\n",
    "    try:\n",
    "        content = file_path.read_text(encoding='utf-8')\n",
    "        data_list.append(json.loads(content))\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "\n",
    "print(len(json_files)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc47128f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk(\"split_dataset\")\n",
    "ro_dataset = DatasetDict({\n",
    "    'train': dataset['train_ro'],\n",
    "    'dev': dataset['dev_ro']\n",
    "})\n",
    "\n",
    "md_dataset = DatasetDict({\n",
    "    'train': dataset['train_md'],\n",
    "    'dev': dataset['dev_md']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32bca307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad81b8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"Helsinki-NLP/opus-mt-ROMANCE-en\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6577f6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = examples[\"source\"]\n",
    "    targets = examples[\"target\"]\n",
    "    \n",
    "    model_inputs = tokenizer(\n",
    "        inputs, \n",
    "        text_target=targets, \n",
    "        max_length=128, \n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "tokenized_ro = ro_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_md = md_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d18711",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_translation_model(train_data, eval_data, model_name_suffix):\n",
    "    \"\"\"\n",
    "    Function to train the model on a specific language split\n",
    "    \"\"\"\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint).to(device)\n",
    "    \n",
    "    peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM, \n",
    "    inference_mode=False, \n",
    "    r=16,           \n",
    "    lora_alpha=32,   \n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"] \n",
    "    )   \n",
    "    \n",
    "    model = get_peft_model(model, peft_config)\n",
    "    \n",
    "    output_dir = f\"./opus-lora-finetuned-{model_name_suffix}\"\n",
    "    final_dir = f\"./final_model_{model_name_suffix}\"\n",
    "\n",
    "    args = Seq2SeqTrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        learning_rate=3e-4,          \n",
    "        per_device_train_batch_size=32, \n",
    "        num_train_epochs=10,       \n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        per_device_eval_batch_size=32,\n",
    "        save_total_limit=1,\n",
    "        predict_with_generate=True,  \n",
    "        fp16=True,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\", \n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=eval_data,\n",
    "        data_collator=data_collator,\n",
    "        processing_class=tokenizer,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "\n",
    "    print(f\"--- Starting training for: {model_name_suffix} ---\")\n",
    "    trainer.train()\n",
    "    trainer.save_model(final_dir)\n",
    "    tokenizer.save_pretrained(final_dir)\n",
    "    print(f\"Model saved to {final_dir}\")\n",
    "    \n",
    "    del model\n",
    "    del trainer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f429bf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_translation_model(tokenized_ro[\"train\"], tokenized_ro[\"dev\"], \"ro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afa5d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2830d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_translation_model(tokenized_md[\"train\"], tokenized_md[\"dev\"], \"md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274add14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "input_text = \"Merg la piață să iau pepene.\" \n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "outputs = model.generate(input_ids=input_ids)\n",
    "decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Source: {input_text}\")\n",
    "print(f\"Translation: {decoded}\")\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ro-Md-En-MT-Analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
