{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41f8f589",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'in1d' from 'numpy.lib._arraysetops_impl' (d:\\Afaculta\\Master\\NMT\\Ro-Md-En-MT-Analysis\\.venv\\Lib\\site-packages\\numpy\\lib\\_arraysetops_impl.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset, DatasetDict\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      5\u001b[39m     AutoTokenizer, \n\u001b[32m      6\u001b[39m     AutoModelForSeq2SeqLM, \n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m     Seq2SeqTrainer\n\u001b[32m     10\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Afaculta\\Master\\NMT\\Ro-Md-En-MT-Analysis\\.venv\\Lib\\site-packages\\torch\\__init__.py:1840\u001b[39m\n\u001b[32m   1837\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_tensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tensor  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[32m   1839\u001b[39m \u001b[38;5;66;03m# needs to be after torch.Tensor is defined to avoid circular dependencies\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1840\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m storage \u001b[38;5;28;01mas\u001b[39;00m storage  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[32m   1841\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstorage\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m   1842\u001b[39m     _LegacyStorage,\n\u001b[32m   1843\u001b[39m     _StorageBase,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1846\u001b[39m     UntypedStorage,\n\u001b[32m   1847\u001b[39m )\n\u001b[32m   1850\u001b[39m \u001b[38;5;66;03m# NOTE: New <type>Storage classes should never be added. When adding a new\u001b[39;00m\n\u001b[32m   1851\u001b[39m \u001b[38;5;66;03m# dtype, use torch.storage.TypedStorage directly.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Afaculta\\Master\\NMT\\Ro-Md-En-MT-Analysis\\.venv\\Lib\\site-packages\\torch\\storage.py:27\u001b[39m\n\u001b[32m     23\u001b[39m __all__ = [\u001b[33m\"\u001b[39m\u001b[33mTypedStorage\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mUntypedStorage\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     29\u001b[39m     HAS_NUMPY = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Afaculta\\Master\\NMT\\Ro-Md-En-MT-Analysis\\.venv\\Lib\\site-packages\\numpy\\__init__.py:471\u001b[39m\n\u001b[32m    469\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m scimath \u001b[38;5;28;01mas\u001b[39;00m emath\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_arraypad_impl\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pad\n\u001b[32m--> \u001b[39m\u001b[32m471\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_arraysetops_impl\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    472\u001b[39m     ediff1d,\n\u001b[32m    473\u001b[39m     in1d,\n\u001b[32m    474\u001b[39m     intersect1d,\n\u001b[32m    475\u001b[39m     isin,\n\u001b[32m    476\u001b[39m     setdiff1d,\n\u001b[32m    477\u001b[39m     setxor1d,\n\u001b[32m    478\u001b[39m     union1d,\n\u001b[32m    479\u001b[39m     unique,\n\u001b[32m    480\u001b[39m     unique_all,\n\u001b[32m    481\u001b[39m     unique_counts,\n\u001b[32m    482\u001b[39m     unique_inverse,\n\u001b[32m    483\u001b[39m     unique_values,\n\u001b[32m    484\u001b[39m )\n\u001b[32m    485\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_function_base_impl\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    486\u001b[39m     angle,\n\u001b[32m    487\u001b[39m     append,\n\u001b[32m   (...)\u001b[39m\u001b[32m    523\u001b[39m     vectorize,\n\u001b[32m    524\u001b[39m )\n\u001b[32m    525\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_histograms_impl\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m histogram, histogram_bin_edges, histogramdd\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'in1d' from 'numpy.lib._arraysetops_impl' (d:\\Afaculta\\Master\\NMT\\Ro-Md-En-MT-Analysis\\.venv\\Lib\\site-packages\\numpy\\lib\\_arraysetops_impl.py)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSeq2SeqLM, \n",
    "    DataCollatorForSeq2Seq, \n",
    "    Seq2SeqTrainingArguments, \n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "from pathlib import Path\n",
    "import os\n",
    "import gdown\n",
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "75cf77e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.12/dist-packages (0.1.1)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from sacremoses) (2025.11.3)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from sacremoses) (8.3.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from sacremoses) (1.5.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sacremoses) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "# only run on colab\n",
    "%pip install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a8af070d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Ro-Md-En-MT-Analysis'...\n",
      "remote: Enumerating objects: 17839, done.\u001b[K\n",
      "remote: Counting objects: 100% (17839/17839), done.\u001b[K\n",
      "remote: Compressing objects: 100% (17822/17822), done.\u001b[K\n",
      "remote: Total 17839 (delta 8), reused 17835 (delta 6), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (17839/17839), 4.46 MiB | 18.96 MiB/s, done.\n",
      "Resolving deltas: 100% (8/8), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/TanaseVictorFlavian/Ro-Md-En-MT-Analysis.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0fbcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Data\n",
    "parallel_corpus_dir = Path.cwd() / \"parallel_corpus\"\n",
    "\n",
    "if parallel_corpus_dir.exists() is False:\n",
    "    # fallback for using colab extension in vscode\n",
    "    parallel_corpus_dir = Path(\"/content/Ro-Md-En-MT-Analysis/parallel_corpus\")\n",
    "\n",
    "json_files = list(parallel_corpus_dir.glob(\"*.json\"))\n",
    "data_list = []\n",
    "\n",
    "for file_path in json_files:\n",
    "    try:\n",
    "        content = file_path.read_text(encoding='utf-8')\n",
    "        data_list.append(json.loads(content))\n",
    "        # am scos chestia cu index = 1000 \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "\n",
    "print(len(json_files)) # sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280238d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = Dataset.from_list(data_list).shuffle(seed=2002) # am adaugat shuffle ca sa nu mai avem prima jumatate ro a doua md\n",
    "split_datasets = raw_dataset.train_test_split(test_size=0.2, seed=42) \n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': split_datasets['train'],\n",
    "    'dev': split_datasets['test']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ab8b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_subset = raw_dataset.select(range(0, 1000))\n",
    "split_subset = raw_subset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "subset = DatasetDict({\n",
    "    \"train\":split_subset['train'],\n",
    "    \"test\":split_subset['test']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "32bca307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ad81b8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"Helsinki-NLP/opus-mt-ROMANCE-en\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6577f6f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f871d3a6f4848a78ecd017387292372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9723a918393340f5aa0243929ace2750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = [ex for ex in examples[\"source\"]]\n",
    "    targets = [ex for ex in examples[\"target\"]]\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True)\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=128, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "tokenized_subsets = subset.map(preprocess_function, batche=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2c519900",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-1729137930.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 01:42, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.860200</td>\n",
       "      <td>0.790198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.651000</td>\n",
       "      <td>0.768445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.541200</td>\n",
       "      <td>0.757784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.461400</td>\n",
       "      <td>0.759584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.409500</td>\n",
       "      <td>0.760808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.363500</td>\n",
       "      <td>0.764995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.327200</td>\n",
       "      <td>0.766895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.307500</td>\n",
       "      <td>0.769391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.294100</td>\n",
       "      <td>0.770834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.284900</td>\n",
       "      <td>0.771506</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py:3918: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[65000]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.encoder.embed_positions.weight', 'model.decoder.embed_tokens.weight', 'model.decoder.embed_positions.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete. Model saved to './final_model'\n"
     ]
    }
   ],
   "source": [
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./opus-mt-finetuned\",\n",
    "    eval_strategy=\"epoch\",      \n",
    "    save_strategy=\"epoch\", \n",
    "    load_best_model_at_end=True,      \n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,         \n",
    "    save_total_limit=1,            \n",
    "    learning_rate=2e-5,               \n",
    "    per_device_train_batch_size=32,   \n",
    "    per_device_eval_batch_size=32,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=10,                \n",
    "    fp16=torch.cuda.is_available(),    \n",
    "    report_to=\"none\",            \n",
    "    logging_strategy=\"epoch\",     \n",
    "    logging_steps=1,                 \n",
    "    logging_dir=None,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"dev\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "trainer.save_model(\"./final_model\")\n",
    "tokenizer.save_pretrained(\"./final_model\")\n",
    "print(\"Training complete. Model saved to './final_model'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "df6304f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: content/final_model/ (stored 0%)\n",
      "  adding: content/final_model/special_tokens_map.json (deflated 35%)\n",
      "  adding: content/final_model/tokenizer_config.json (deflated 62%)\n",
      "  adding: content/final_model/training_args.bin (deflated 53%)\n",
      "  adding: content/final_model/config.json (deflated 63%)\n",
      "  adding: content/final_model/source.spm (deflated 49%)\n",
      "  adding: content/final_model/target.spm (deflated 50%)\n",
      "  adding: content/final_model/vocab.json (deflated 70%)\n",
      "  adding: content/final_model/model.safetensors (deflated 7%)\n",
      "  adding: content/final_model/generation_config.json (deflated 40%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r model_output.zip /content/final_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274add14",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Merg la piață să iau pepene.\" \n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "outputs = model.generate(input_ids)\n",
    "decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Source: {input_text}\")\n",
    "print(f\"Translation: {decoded}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
