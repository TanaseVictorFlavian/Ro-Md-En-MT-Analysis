{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f8f589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSeq2SeqLM, \n",
    "    DataCollatorForSeq2Seq, \n",
    "    Seq2SeqTrainingArguments, \n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "from pathlib import Path\n",
    "import os\n",
    "import gdown\n",
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "75cf77e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.12/dist-packages (0.1.1)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from sacremoses) (2025.11.3)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from sacremoses) (8.3.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from sacremoses) (1.5.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sacremoses) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "# only run on colab\n",
    "%pip install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a8af070d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Ro-Md-En-MT-Analysis'...\n",
      "remote: Enumerating objects: 17839, done.\u001b[K\n",
      "remote: Counting objects: 100% (17839/17839), done.\u001b[K\n",
      "remote: Compressing objects: 100% (17822/17822), done.\u001b[K\n",
      "remote: Total 17839 (delta 8), reused 17835 (delta 6), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (17839/17839), 4.46 MiB | 18.96 MiB/s, done.\n",
      "Resolving deltas: 100% (8/8), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/TanaseVictorFlavian/Ro-Md-En-MT-Analysis.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0fbcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Data\n",
    "parallel_corpus_dir = Path.cwd() / \"parallel_corpus\"\n",
    "\n",
    "if parallel_corpus_dir.exists() is False:\n",
    "    # fallback for using colab extension in vscode\n",
    "    parallel_corpus_dir = Path(\"/content/Ro-Md-En-MT-Analysis/parallel_corpus\")\n",
    "\n",
    "json_files = list(parallel_corpus_dir.glob(\"*.json\"))\n",
    "data_list = []\n",
    "\n",
    "for index, file_path in enumerate(json_files):\n",
    "    try:\n",
    "        content = file_path.read_text(encoding='utf-8')\n",
    "        if index == 1000:\n",
    "            break\n",
    "        data_list.append(json.loads(content))\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "280238d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = Dataset.from_list(data_list)\n",
    "split_datasets = raw_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': split_datasets['train'],\n",
    "    'dev': split_datasets['test']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "32bca307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ad81b8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"Helsinki-NLP/opus-mt-ROMANCE-en\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6577f6f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f871d3a6f4848a78ecd017387292372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9723a918393340f5aa0243929ace2750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = [ex for ex in examples[\"source\"]]\n",
    "    targets = [ex for ex in examples[\"target\"]]\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True)\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=128, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2c519900",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-1729137930.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 01:42, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.860200</td>\n",
       "      <td>0.790198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.651000</td>\n",
       "      <td>0.768445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.541200</td>\n",
       "      <td>0.757784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.461400</td>\n",
       "      <td>0.759584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.409500</td>\n",
       "      <td>0.760808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.363500</td>\n",
       "      <td>0.764995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.327200</td>\n",
       "      <td>0.766895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.307500</td>\n",
       "      <td>0.769391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.294100</td>\n",
       "      <td>0.770834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.284900</td>\n",
       "      <td>0.771506</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py:3918: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[65000]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.encoder.embed_positions.weight', 'model.decoder.embed_tokens.weight', 'model.decoder.embed_positions.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete. Model saved to './final_model'\n"
     ]
    }
   ],
   "source": [
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./opus-mt-finetuned\",\n",
    "    eval_strategy=\"epoch\",      \n",
    "    save_strategy=\"epoch\", \n",
    "    load_best_model_at_end=True,      \n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,         \n",
    "    save_total_limit=1,            \n",
    "    learning_rate=2e-5,               \n",
    "    per_device_train_batch_size=32,   \n",
    "    per_device_eval_batch_size=32,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=10,                \n",
    "    fp16=torch.cuda.is_available(),    \n",
    "    report_to=\"none\",            \n",
    "    logging_strategy=\"epoch\",     \n",
    "    logging_steps=1,                 \n",
    "    logging_dir=None,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"dev\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "trainer.save_model(\"./final_model\")\n",
    "tokenizer.save_pretrained(\"./final_model\")\n",
    "print(\"Training complete. Model saved to './final_model'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "df6304f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: content/final_model/ (stored 0%)\n",
      "  adding: content/final_model/special_tokens_map.json (deflated 35%)\n",
      "  adding: content/final_model/tokenizer_config.json (deflated 62%)\n",
      "  adding: content/final_model/training_args.bin (deflated 53%)\n",
      "  adding: content/final_model/config.json (deflated 63%)\n",
      "  adding: content/final_model/source.spm (deflated 49%)\n",
      "  adding: content/final_model/target.spm (deflated 50%)\n",
      "  adding: content/final_model/vocab.json (deflated 70%)\n",
      "  adding: content/final_model/model.safetensors (deflated 7%)\n",
      "  adding: content/final_model/generation_config.json (deflated 40%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r model_output.zip /content/final_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274add14",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Merg la piață să iau pepene.\" \n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "outputs = model.generate(input_ids)\n",
    "decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Source: {input_text}\")\n",
    "print(f\"Translation: {decoded}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
