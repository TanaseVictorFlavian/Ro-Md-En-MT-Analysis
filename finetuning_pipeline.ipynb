{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41f8f589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict)\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from pathlib import Path\n",
    "import os\n",
    "import gdown\n",
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75cf77e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from sacremoses) (2025.11.3)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from sacremoses) (8.3.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from sacremoses) (1.5.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sacremoses) (4.67.1)\n",
      "Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sacremoses\n",
      "Successfully installed sacremoses-0.1.1\n"
     ]
    }
   ],
   "source": [
    "# only run on colab\n",
    "%pip install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8af070d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Ro-Md-En-MT-Analysis'...\n",
      "remote: Enumerating objects: 30044, done.\u001b[K\n",
      "remote: Counting objects: 100% (30044/30044), done.\u001b[K\n",
      "remote: Compressing objects: 100% (30017/30017), done.\u001b[K\n",
      "remote: Total 30044 (delta 19), reused 30037 (delta 14), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (30044/30044), 6.71 MiB | 11.18 MiB/s, done.\n",
      "Resolving deltas: 100% (19/19), done.\n",
      "Updating files: 100% (30012/30012), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/TanaseVictorFlavian/Ro-Md-En-MT-Analysis.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d0fbcd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n"
     ]
    }
   ],
   "source": [
    "# Loading Data\n",
    "parallel_corpus_dir = Path.cwd() / \"parallel_corpus\"\n",
    "\n",
    "if parallel_corpus_dir.exists() is False:\n",
    "    # fallback for using colab extension in vscode\n",
    "    parallel_corpus_dir = Path(\"/content/Ro-Md-En-MT-Analysis/parallel_corpus\")\n",
    "\n",
    "json_files = list(parallel_corpus_dir.glob(\"*.json\"))\n",
    "data_list = []\n",
    "\n",
    "for file_path in json_files:\n",
    "    try:\n",
    "        content = file_path.read_text(encoding='utf-8')\n",
    "        data_list.append(json.loads(content))\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "\n",
    "print(len(json_files)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "280238d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = Dataset.from_list(data_list).shuffle(seed=2002) \n",
    "split_datasets = raw_dataset.train_test_split(test_size=0.2, seed=42) \n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': split_datasets['train'],\n",
    "    'dev': split_datasets['test']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ab8b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "raw_subset = raw_dataset.select(range(0, 1000))\n",
    "split_subset = raw_subset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "subset = DatasetDict({\n",
    "    \"train\":split_subset['train'],\n",
    "    \"test\":split_subset['test']\n",
    "})\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32bca307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad81b8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"Helsinki-NLP/opus-mt-ROMANCE-en\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6577f6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = examples[\"source\"]\n",
    "    targets = examples[\"target\"]\n",
    "    \n",
    "    model_inputs = tokenizer(\n",
    "        inputs, \n",
    "        text_target=targets, \n",
    "        max_length=128, \n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b280739c",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM, \n",
    "    inference_mode=False, \n",
    "    r=16,           \n",
    "    lora_alpha=32,   \n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"] \n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c519900",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-1729137930.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 01:42, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.860200</td>\n",
       "      <td>0.790198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.651000</td>\n",
       "      <td>0.768445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.541200</td>\n",
       "      <td>0.757784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.461400</td>\n",
       "      <td>0.759584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.409500</td>\n",
       "      <td>0.760808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.363500</td>\n",
       "      <td>0.764995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.327200</td>\n",
       "      <td>0.766895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.307500</td>\n",
       "      <td>0.769391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.294100</td>\n",
       "      <td>0.770834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.284900</td>\n",
       "      <td>0.771506</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py:3918: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[65000]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.encoder.embed_positions.weight', 'model.decoder.embed_tokens.weight', 'model.decoder.embed_positions.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete. Model saved to './final_model'\n"
     ]
    }
   ],
   "source": [
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./opus-lora-finetuned\",\n",
    "    learning_rate=3e-4,          \n",
    "    per_device_train_batch_size=32, \n",
    "    num_train_epochs=10,       \n",
    "    \n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_eval_batch_size=32,\n",
    "    save_total_limit=1,\n",
    "    predict_with_generate=True,  \n",
    "    fp16=True,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"dev\"],\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "print(\"Start training...\")\n",
    "trainer.train()\n",
    "trainer.save_model(\"./final_model\")\n",
    "tokenizer.save_pretrained(\"./final_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274add14",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Merg la piață să iau pepene.\" \n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "outputs = model.generate(input_ids=input_ids)\n",
    "decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Source: {input_text}\")\n",
    "print(f\"Translation: {decoded}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
