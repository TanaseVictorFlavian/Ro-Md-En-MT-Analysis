{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8981f8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model on cuda...\n",
      "Input:  Salut, lume!\n",
      "Output: Hello, world!\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from pathlib import Path\n",
    "from datasets import Dataset, DatasetDict, load_from_disk\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from peft import PeftModel\n",
    "\n",
    "\n",
    "MODEL_NAME = \"Helsinki-NLP/opus-mt-ROMANCE-en\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 32  # Adjust based on your GPU VRAM\n",
    "\n",
    "# --- Load Resources ---\n",
    "print(f\"Loading model on {DEVICE}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "\n",
    "adapter_path = \"models\\model_opus_ro\"\n",
    "model = PeftModel.from_pretrained(model, adapter_path)\n",
    "\n",
    "input_text = \"Salut, lume!\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(DEVICE)\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=60)\n",
    "\n",
    "# Decode (Turn numbers back into words)\n",
    "translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Input:  {input_text}\")\n",
    "print(f\"Output: {translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79824f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model on cuda...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing dev_ro ---\n",
      "Generating translations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 422/422 [05:53<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Translation Metrics (ChrF++, TER, BERTScore)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dev_ro] ChrF++: 78.19\n",
      "[dev_ro] TER: 25.32\n",
      "[dev_ro] BERTScore F1: 0.9725\n",
      "Computing Linguistic Metrics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating NER: 100%|██████████| 422/422 [03:21<00:00,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dev_ro] Perplexity: 3252.28 ± 3588585.80\n",
      "[dev_ro] Sentence Len: 13.59 ± 25.76\n",
      "[dev_ro] NER Recall: 0.7324\n",
      "\n",
      "--- Processing dev_md ---\n",
      "Generating translations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 422/422 [06:29<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Translation Metrics (ChrF++, TER, BERTScore)...\n",
      "[dev_md] ChrF++: 73.99\n",
      "[dev_md] TER: 31.55\n",
      "[dev_md] BERTScore F1: 0.9662\n",
      "Computing Linguistic Metrics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating NER: 100%|██████████| 422/422 [06:53<00:00,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dev_md] Perplexity: 3201.16 ± 4007790.54\n",
      "[dev_md] Sentence Len: 13.93 ± 25.18\n",
      "[dev_md] NER Recall: 0.6358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datasets import load_from_disk\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline,DataCollatorForSeq2Seq\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Configuration ---\n",
    "MODEL_NAME = \"Helsinki-NLP/opus-mt-ROMANCE-en\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 32  # Adjust based on your GPU VRAM\n",
    "\n",
    "# --- Load Resources ---\n",
    "print(f\"Loading model on {DEVICE}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "\n",
    "adapter_path = \"models\\model_opus_ro\"\n",
    "model = PeftModel.from_pretrained(model, adapter_path)\n",
    "\n",
    "# Load Metrics\n",
    "metric_chrf = evaluate.load(\"chrf\")\n",
    "metric_ter = evaluate.load(\"ter\")\n",
    "metric_bertscore = evaluate.load(\"bertscore\")\n",
    "# NER Pipeline for English (since output is English)\n",
    "ner_pipe = pipeline(\"ner\", model=\"dslim/bert-base-NER\", aggregation_strategy=\"simple\", device=0 if DEVICE==\"cuda\" else -1)\n",
    "\n",
    "# Load Data\n",
    "base_dir = Path.cwd()\n",
    "dataset = load_from_disk(\"split_dataset\")\n",
    "\n",
    "def calculate_perplexity(model, tokenizer, sources, references):\n",
    "    model.eval()\n",
    "    \n",
    "    # Use the same collator as training to guarantee identical behavior\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=True)\n",
    "    batch_losses = []\n",
    "    \n",
    "    for i in range(0, len(sources), BATCH_SIZE):\n",
    "        batch_src = sources[i:i+BATCH_SIZE]\n",
    "        batch_ref = references[i:i+BATCH_SIZE]\n",
    "\n",
    "        inputs = tokenizer(batch_src, truncation=True, max_length=128)\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(batch_ref, truncation=True, max_length=128)\n",
    "            \n",
    "        features = []\n",
    "        for j in range(len(batch_src)):\n",
    "            features.append({\n",
    "                \"input_ids\": inputs[\"input_ids\"][j],\n",
    "                \"attention_mask\": inputs[\"attention_mask\"][j],\n",
    "                \"labels\": labels[\"input_ids\"][j]\n",
    "            })\n",
    "            \n",
    "        batch = data_collator(features)\n",
    "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "            batch_losses.append(outputs.loss.item())\n",
    "\n",
    "    # Calculate perplexity by taking the mean of losses first, then exponentiate\n",
    "    mean_loss = np.mean(batch_losses)\n",
    "    final_ppl = np.exp(mean_loss)\n",
    "    \n",
    "    # Calculate variance \n",
    "    batch_ppls = [np.exp(loss) for loss in batch_losses]\n",
    "    ppl_var = np.var(batch_ppls)\n",
    "    \n",
    "    return final_ppl, ppl_var\n",
    "\n",
    "def calculate_ner_recall(references, predictions):\n",
    "    \"\"\"\n",
    "    Calculates NER Recall: What % of entities in Reference were found in Prediction?\n",
    "    \"\"\"\n",
    "    total_recall_scores = []\n",
    "    \n",
    "    # Process in batches to speed up NER pipeline\n",
    "    for i in tqdm(range(0, len(references), BATCH_SIZE), desc=\"Calculating NER\"):\n",
    "        batch_refs = references[i:i+BATCH_SIZE]\n",
    "        batch_preds = predictions[i:i+BATCH_SIZE]\n",
    "        \n",
    "        # Get entities\n",
    "        ref_entities_batch = ner_pipe(batch_refs)\n",
    "        pred_entities_batch = ner_pipe(batch_preds)\n",
    "        \n",
    "        for ref_ents, pred_ents in zip(ref_entities_batch, pred_entities_batch):\n",
    "            # Extract simple set of entity text (lowercase to be forgiving)\n",
    "            ref_set = {ent['word'].lower() for ent in ref_ents}\n",
    "            pred_set = {ent['word'].lower() for ent in pred_ents}\n",
    "            \n",
    "            if len(ref_set) == 0:\n",
    "                continue # Skip sentences with no named entities in reference\n",
    "                \n",
    "            # Intersection\n",
    "            matched = ref_set.intersection(pred_set)\n",
    "            recall = len(matched) / len(ref_set)\n",
    "            total_recall_scores.append(recall)\n",
    "            \n",
    "    return np.mean(total_recall_scores) if total_recall_scores else 0.0\n",
    "\n",
    "def run_evaluation(split_name, data):\n",
    "    print(f\"\\n--- Processing {split_name} ---\")\n",
    "    \n",
    "    # 1. Extract Source and References\n",
    "    # ASSUMPTION: Dataset has 'ro'/'md' column or 'source', and 'en' or 'target'\n",
    "    # Adjust these keys based on your actual dataset structure!\n",
    "    src_key = 'ro' if 'ro' in data.column_names else 'md' \n",
    "    if src_key not in data.column_names: src_key = 'source' # fallback\n",
    "    ref_key = 'en' if 'en' in data.column_names else 'target'\n",
    "    \n",
    "    sources = data[src_key]\n",
    "    references = data[ref_key]\n",
    "    \n",
    "    # 2. Generate Translations (Hypotheses)\n",
    "    print(\"Generating translations...\")\n",
    "    hypotheses = []\n",
    "    for i in tqdm(range(0, len(sources), BATCH_SIZE)):\n",
    "        batch = sources[i:i+BATCH_SIZE]\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True).to(DEVICE)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "        \n",
    "        decoded = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        hypotheses.extend(decoded)\n",
    "\n",
    "    # --- PIPELINE 1: Translation Metrics ---\n",
    "    print(\"Computing Translation Metrics (ChrF++, TER, BERTScore)...\")\n",
    "    \n",
    "    score_chrf = metric_chrf.compute(predictions=hypotheses, references=references, word_order=2) # word_order=2 enables chrf++\n",
    "    score_ter = metric_ter.compute(predictions=hypotheses, references=references)\n",
    "    score_bert = metric_bertscore.compute(predictions=hypotheses, references=references, lang=\"en\")\n",
    "    \n",
    "    print(f\"[{split_name}] ChrF++: {score_chrf['score']:.2f}\")\n",
    "    print(f\"[{split_name}] TER: {score_ter['score']:.2f}\")\n",
    "    print(f\"[{split_name}] BERTScore F1: {np.mean(score_bert['f1']):.4f}\")\n",
    "\n",
    "    # --- PIPELINE 2: Linguistic Analysis ---\n",
    "    print(\"Computing Linguistic Metrics...\")\n",
    "    \n",
    "    # Perplexity (Reference given Source)\n",
    "    ppl_mean, ppl_var = calculate_perplexity(model, tokenizer, sources, references)\n",
    "    \n",
    "    # Sentence Length (of Translations)\n",
    "    lens = [len(x.split()) for x in hypotheses]\n",
    "    len_mean = np.mean(lens)\n",
    "    len_var = np.var(lens)\n",
    "    \n",
    "    # NER Recall\n",
    "    ner_rec = calculate_ner_recall(references, hypotheses)\n",
    "    \n",
    "    print(f\"[{split_name}] Perplexity: {ppl_mean:.2f} ± {ppl_var:.2f}\")\n",
    "    print(f\"[{split_name}] Sentence Len: {len_mean:.2f} ± {len_var:.2f}\")\n",
    "    print(f\"[{split_name}] NER Recall: {ner_rec:.4f}\")\n",
    "\n",
    "# --- Run ---\n",
    "run_evaluation(\"dev_ro\", dataset[\"train_ro\"])\n",
    "run_evaluation(\"dev_md\", dataset[\"train_md\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
